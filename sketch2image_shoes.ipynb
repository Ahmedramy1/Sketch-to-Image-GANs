{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xAxzH_UNHIE",
    "outputId": "2e5e072e-c436-4fcc-d4a5-d48e4c867dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  7 07:28:06 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 511.79       Driver Version: 511.79       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   54C    P8     8W /  N/A |    717MiB /  8192MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      3028    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A      3520    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A      8280    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A      9104    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A      9900    C+G   ...ser\\Application\\brave.exe    N/A      |\n",
      "|    0   N/A  N/A     10876    C+G                                   N/A      |\n",
      "|    0   N/A  N/A     11028    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     11396    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     11424    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     12480    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     13128    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     13924    C+G                                   N/A      |\n",
      "|    0   N/A  N/A     14720    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\n",
      "|    0   N/A  N/A     14832    C+G                                   N/A      |\n",
      "|    0   N/A  N/A     15484    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     15980    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "|    0   N/A  N/A     16924    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     20060    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "    print('Not connected to a GPU')\n",
    "else:\n",
    "    print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NX3ZgaT76rzb"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Model, Input, load_model\n",
    "from keras.layers import Conv2D, Conv2DTranspose, LeakyReLU, Activation, Concatenate ,Dropout, BatchNormalization\n",
    "from keras.preprocessing import image\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join, isfile\n",
    "import tensorflow as tf\n",
    "from csv import writer\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Downloads\\edges2shoes\\edges2shoes\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, size = (256,256)):\n",
    "    imgs_list = list()\n",
    "    for filename in tqdm(listdir(path)):\n",
    "        img = image.load_img(path + filename, target_size=size)      \n",
    "        img = image.img_to_array(img)\n",
    "        imgs_list.append(img)\n",
    "    return [np.asarray(imgs_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 8000/8000 [00:13<00:00, 589.22it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 8000/8000 [00:13<00:00, 582.51it/s]\n"
     ]
    }
   ],
   "source": [
    "[src_images] = load_image('Augmented_Shoes_S/')\n",
    "[tar_images] = load_image('Augmented_Shoes_I/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 256, 256, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 256, 256, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Pl8zKcBO73ck"
   },
   "outputs": [],
   "source": [
    "def discriminator(image_shape):\n",
    "\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "\n",
    "    in_src_image = Input(shape=image_shape)\n",
    "\n",
    "    in_target_image = Input(shape=image_shape)\n",
    "\n",
    "    merged = Concatenate()([in_src_image, in_target_image])\n",
    "\n",
    "    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    d = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "    patch_out = Activation('sigmoid')(d)\n",
    "\n",
    "    model = Model([in_src_image, in_target_image], patch_out)\n",
    "\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(layer_in, n_filters, batchnorm=True):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    g = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "    if batchnorm:\n",
    "        g = BatchNormalization()(g, training=True)\n",
    "    g = LeakyReLU(alpha=0.2)(g)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    g = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "    g = BatchNormalization()(g, training=True)\n",
    "\n",
    "    if dropout:\n",
    "        g = Dropout(0.5)(g, training=True)\n",
    "    #print(g.shape)\n",
    "    #print(skip_in.shape)\n",
    "    g = Concatenate()([g, skip_in])\n",
    "    g = Activation('relu')(g)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tQOjSzlY94Jz"
   },
   "outputs": [],
   "source": [
    "def generator(image_shape=(256,256,3)):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    in_image = Input(shape=image_shape)\n",
    "    # encoder model\n",
    "    e1 = encoder_block(in_image, 64, batchnorm=False)\n",
    "    e2 = encoder_block(e1, 128)\n",
    "    e3 = encoder_block(e2, 256)\n",
    "    e4 = encoder_block(e3, 512)\n",
    "    e5 = encoder_block(e4, 512)\n",
    "    e6 = encoder_block(e5, 512)\n",
    "    e7 = encoder_block(e6, 512)\n",
    "    # bottleneck, no batch norm and relu\n",
    "    b = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n",
    "    b = Activation('relu')(b)\n",
    "    # decoder model\n",
    "    d1 = decoder_block(b, e7, 512)\n",
    "    d2 = decoder_block(d1, e6, 512)\n",
    "    d3 = decoder_block(d2, e5, 512)\n",
    "    d4 = decoder_block(d3, e4, 512, dropout=False)\n",
    "    d5 = decoder_block(d4, e3, 256, dropout=False)\n",
    "    d6 = decoder_block(d5, e2, 128, dropout=False)\n",
    "    d7 = decoder_block(d6, e1, 64, dropout=False)\n",
    "    # output\n",
    "    g = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n",
    "    out_image = Activation('tanh')(g)\n",
    "    # define model\n",
    "    model = Model(in_image, out_image)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gan model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1pPfLLY094fP"
   },
   "outputs": [],
   "source": [
    "def gan(g_model, d_model, image_shape):\n",
    "    d_model.trainable = False\n",
    "    in_src = Input(shape=image_shape)\n",
    "    gen_out = g_model(in_src)\n",
    "    dis_out = d_model([in_src, gen_out])\n",
    "    model = Model(in_src, [dis_out, gen_out])\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mt40E0eb95G1"
   },
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples, patch_shape):\n",
    "    # unpack dataset\n",
    "    trainA, trainB = dataset\n",
    "    ix = np.random.randint(0, trainA.shape[0], n_samples)\n",
    "    X1, X2 = trainA[ix], trainB[ix]\n",
    "    y = np.ones((n_samples, patch_shape, patch_shape, 1))\n",
    "    return [X1, X2], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rXv5pwdf95Wu"
   },
   "outputs": [],
   "source": [
    "def generate_fake_samples(g_model, samples, patch_shape):\n",
    "    X = g_model.predict(samples)\n",
    "    y = np.zeros((len(X), patch_shape, patch_shape, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Downloads\\edges2shoes\\edges2shoes\n"
     ]
    }
   ],
   "source": [
    "# Print directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_y5u3Mxp95kj"
   },
   "outputs": [],
   "source": [
    "def summarize_performance(step, g_model, dataset, n_samples=3):\n",
    "    [X_realA, X_realB], _ = generate_real_samples(dataset, n_samples, 1)\n",
    "    X_fakeB, _ = generate_fake_samples(g_model, X_realA, 1)\n",
    "    X_realA = (X_realA + 1) / 2.0\n",
    "    X_realB = (X_realB + 1) / 2.0\n",
    "    X_fakeB = (X_fakeB + 1) / 2.0\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(3, n_samples, 1 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X_realA[i])\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(3, n_samples, 1 + n_samples + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X_fakeB[i])\n",
    "    # plot real target image\n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X_realB[i])\n",
    "    # save plot to file\n",
    "    filename1 = 'tests/plot_ck_%07d.png' % (step+1)\n",
    "    pyplot.savefig(filename1)\n",
    "    pyplot.close()\n",
    "    print('>Saved: %s' % (filename1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "IzFQK-A5952f"
   },
   "outputs": [],
   "source": [
    "def train(d_model, g_model, gan_model, dataset, n_epochs=35, n_batch=8):\n",
    "    \n",
    "    generator_optimizer = Adam(learning_rate = 0.0002, beta_1=0.5)\n",
    "    discriminator_optimizer = Adam(learning_rate = 0.0002, beta_1=0.5)\n",
    "    checkpoint_dir = 'Shoes_Checkpoints'\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "    checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer, \n",
    "                                     discriminator_optimizer=discriminator_optimizer, \n",
    "                                     generator=g_model, discriminator=d_model, \n",
    "                                     gan_model = gan_model)\n",
    "    ckpt_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=3)\n",
    "    if ckpt_manager.latest_checkpoint:\n",
    "        checkpoint.restore(ckpt_manager.latest_checkpoint)  #ckpt_manager.checkpoints[3]\n",
    "        print('Latest checkpoint restored!')\n",
    "    else:\n",
    "        print('No Previous Checkpoints')\n",
    "    \n",
    "    n_patch = d_model.output_shape[1]\n",
    "    trainA, trainB = dataset\n",
    "    bat_per_epo = int(len(trainA) / n_batch)\n",
    "    print(\"Batch Per Epoch: {}\".format(bat_per_epo))\n",
    "    print(\"Length of TrainA: {}\".format(len(trainA)))\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    for i in tqdm(range(n_steps)):\n",
    "        [X_realA, X_realB], y_real = generate_real_samples(dataset, n_batch, n_patch)\n",
    "        X_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n",
    "        d_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
    "        d_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n",
    "        g_loss, _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB])\n",
    "        if i % 300 == 0:\n",
    "            print('>%d, d1[%.5f] d2[%.5f] g[%.5f]' % (i+1, d_loss1, d_loss2, g_loss))\n",
    "            with open('logs.csv', 'a', newline='') as f_object:  \n",
    "                writer_object = writer(f_object)\n",
    "                writer_object.writerow([i+1, d_loss1, d_loss2, g_loss])  \n",
    "                f_object.close()\n",
    "        if (i+1) % (bat_per_epo * 2) == 0:\n",
    "            summarize_performance(i, g_model, dataset)\n",
    "            ckpt_save_path = ckpt_manager.save()\n",
    "            print ('Saving checkpoint for epoch {} at {}'.format(i+1,ckpt_save_path))\n",
    "            g_model.save('Shoes_Checkpoints/model_aug_shoes_%07d.h5' % (i+1))\n",
    "            print('>Saved g_model: %s ' % ('model_aug_shoes_%07d.h5' % (i+1)))\n",
    "            print('Epoch %d' % int(i/3000))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Real and Sketch images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_images = (src_images - 127.5) / 127.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_images = (tar_images - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gm__th7396Fu",
    "outputId": "4c74ca97-1eda-406b-d5f6-31333c201090",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded (8000, 256, 256, 3) (8000, 256, 256, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!\n",
      "Batch Per Epoch: 1000\n",
      "Length of TrainA: 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 1/35000 [00:18<178:05:34, 18.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, d1[0.00000] d2[0.00001] g[21.19373]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                                           | 301/35000 [02:53<5:09:57,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">301, d1[0.00001] d2[0.00000] g[22.66372]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▎                                                                          | 601/35000 [05:30<4:56:27,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">601, d1[0.00001] d2[0.00000] g[15.96069]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▉                                                                          | 901/35000 [08:11<5:05:24,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">901, d1[0.00000] d2[0.00000] g[13.14638]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▌                                                                        | 1201/35000 [10:51<4:49:16,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1201, d1[0.00000] d2[0.00000] g[10.50205]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███                                                                        | 1412/35000 [12:45<4:54:05,  1.90it/s]"
     ]
    }
   ],
   "source": [
    "dataset = [src_images, tar_images]\n",
    "print('Loaded', dataset[0].shape, dataset[1].shape)\n",
    "image_shape = dataset[0].shape[1:]\n",
    "# define the models\n",
    "d_model = discriminator(image_shape)\n",
    "g_model = generator(image_shape)\n",
    "# define the gan model\n",
    "gan_model = gan(g_model, d_model, image_shape)\n",
    "# train model\n",
    "train(d_model, g_model, gan_model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnum = 6000\\nmpath = 'Checkpoints/model_ck_000'\\ngt = False\\nfor i in range(7):\\n    if num >= 9999 and not gt:\\n        mpath = mpath[:-1]\\n        gt = True\\n    mm = load_model(mpath+ str(num) + '.h5')\\n    summarize_performance(num-1, mm, dataset)\\n    num += 3000\\n    \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "num = 6000\n",
    "mpath = 'Checkpoints/model_ck_000'\n",
    "gt = False\n",
    "for i in range(7):\n",
    "    if num >= 9999 and not gt:\n",
    "        mpath = mpath[:-1]\n",
    "        gt = True\n",
    "    mm = load_model(mpath+ str(num) + '.h5')\n",
    "    summarize_performance(num-1, mm, dataset)\n",
    "    num += 3000\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z6MnSH0e97KW",
    "outputId": "f85cb4fb-dbaf-45d4-eef2-7133bd25dbf5"
   },
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "F6Y7bkXP979a",
    "outputId": "d516d890-e122-41fc-a904-79344c867dae"
   },
   "outputs": [],
   "source": [
    "#plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qeNBPQnQ99GU"
   },
   "outputs": [],
   "source": [
    "#!pip install pydot \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sketch2image.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
